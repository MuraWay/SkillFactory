{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jkAnkDOi_kOW"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tnrange\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfstFsTl_kOa"
   },
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GjTLWZo2_kOb"
   },
   "outputs": [],
   "source": [
    "def getData(brands, start_year, finish_year, list_feature):\n",
    "    '''\n",
    "    Возвращает датафрейм со спарсенными данными\n",
    "    brands (str/list) - список марок автомобилей для парсинга\n",
    "    min_year (int) - минимальный год выпуска авто, который нас интересует\n",
    "    list_feature (list) - список признаков для парсинга\n",
    "\n",
    "    Ограничение avto.ru - 3700 ссылок за 1 раз, поэтому по каждому бренду и \n",
    "    году будем делать отдельный запрос\n",
    "    \n",
    "    Задаём начальный фильт выдачи:\n",
    "    с пробегом, фото - не важно, таможня - не важно, состояние не важно\n",
    "    ссылка копируется с сайта\n",
    "    '''       \n",
    "    # начальная страница - фильтр\n",
    "    url = 'https://auto.ru/cars/used/?damage_group=ANY&customs_state_group\\=DOESNT_MATTER&has_image=false'\n",
    "\n",
    "    # если вдруг на входе одна марка, а не список \n",
    "    if type(brands)==str:\n",
    "        brands = brands.split()\n",
    "\n",
    "    # иницализируем датафрейм\n",
    "    df = pd.DataFrame(columns=list_feature)\n",
    "    \n",
    "    # на случай, если парсить по возрастанию или убыванию даты выпуска\n",
    "    if start_year > finish_year:\n",
    "        step = -1\n",
    "    else: \n",
    "        step = 1\n",
    "    finish_year += step\n",
    "\n",
    "    # визуализируем прогресс выполнения\n",
    "    print(list_brands)\n",
    "    with tnrange(len(list_brands)) as brands:\n",
    "        for brand in brands:\n",
    "            brands.set_description(f'Общий ход выполнения')\n",
    "        \n",
    "            # Собираем все ссылки по бренду в заданном интервале дат:\n",
    "            all_links = getAllLinksBrand(url, start_year, finish_year, step, list_brands[brand])\n",
    "    \n",
    "            with tnrange(len(all_links), leave=True, unit=' url ') as links:\n",
    "                for lks in links:\n",
    "                    links.set_description(f'{list_brands[brand]}')\n",
    "                    \n",
    "                    # переходим по найденным на странице ссылкам собираем данные\n",
    "                    data_row = getAutoData(all_links[lks], list_feature)\n",
    "                    df = df.append(data_row, ignore_index=True)\n",
    "\n",
    "                # Сохраняем каждую марку в отдельный файл\n",
    "                now = datetime.datetime.now()\n",
    "                t = f'{now:%Y_%m_%d_%H_%M}'\n",
    "                file_name = f'parsing _{list_brands[brand]}_{start_year}_{finish_year}_{t}'\n",
    "            \n",
    "            # df.to_csv(file_name + '.csv', sep='\\t', index=False, encoding='utf-8')\n",
    "            SaveToExcel(df, file_name + '.xlsx')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3-79fEzd_kOc"
   },
   "outputs": [],
   "source": [
    "def getAllLinksBrand(url, start_year, finish_year, step, brand):\n",
    "    '''\n",
    "    возвращает список ссылок по бренду в интервале дат\n",
    "    '''\n",
    "    all_links = [] # список для всех ссылок по бренду\n",
    "    # из-за ограничения собираем все возможные ссылки по 1 бренду и году за раз\n",
    "    \n",
    "\n",
    "    with tnrange(start_year,finish_year, step, leave=False) as years:\n",
    "        for year in years:\n",
    "            years.set_description(f'{year}')\n",
    "            # динамическая часть ссылки\n",
    "            part = str(brand).lower() + '/' + str(year) + '-year'\n",
    "            # собираем ссылку для поиска по одному бренду и году \n",
    "            link = url.replace('cars/used','cars/' + part + '/used')\n",
    "            \n",
    "            # собираем ссылки с каждой страницы выдачи (максимум - 99 страниц)\n",
    "            for n_page in range(1,100):\n",
    "                page_links = getPageLinks(link, n_page) # список ссылок с текущей страницы\n",
    "                if len(page_links) == 0: # выходим если не нашли ссылок\n",
    "                    break\n",
    "                else:\n",
    "                    all_links += page_links\n",
    "    return (all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7OLIncTF_kOc"
   },
   "outputs": [],
   "source": [
    "def getAutoData(auto_page, list_feature):\n",
    "    '''\n",
    "    Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
    "    auto_page (string) ссылка на страницу с объявлением\n",
    "    '''   \n",
    "    # словарь, для храения значений спарсенных значений признаков\n",
    "    dict_row = {}\n",
    "    \n",
    "    # запрашиваем данные по ссылке , headers={'User-Agent': UserAgent().chrome}\n",
    "    response = requests.get(auto_page)\n",
    "    \n",
    "    if not response.ok: # если сервер нам отказал, вернем статус ошибки \n",
    "        print(f'Сервер: {response.status_code}, {auto_page}')\n",
    "    else:  \n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "        # динамический список признаков для парсинга\n",
    "        list_snippet = set(list_feature.copy())\n",
    "\n",
    "        # используем функции для парсинга и вручную добавляем еще 2\n",
    "        for func in range(6):\n",
    "            if func == 0:\n",
    "                dict_part_row = {'color':getColor(soup)}\n",
    "            if func == 1:\n",
    "                dict_part_row = {'image':getFoto(soup)}\n",
    "            if func == 2:\n",
    "                dict_part_row = getSkript_initial_state(soup, list_snippet, auto_page)\n",
    "            elif func == 3:\n",
    "                dict_part_row = getSkript_application_ld_json(soup, list_snippet, auto_page)\n",
    "            elif func == 4:\n",
    "                dict_part_row = getСharacteristics(soup, list_snippet)\n",
    "            else:\n",
    "                timestamp = int(time.time())\n",
    "                dict_part_row = {'car_url':auto_page,\n",
    "                                 'parsing_unixtime':timestamp}\n",
    "\n",
    "            # исключим из списка ту часть признаков, для которой получены значения\n",
    "            llist = set(list(dict_part_row.keys()))\n",
    "            list_snippet -= llist\n",
    "\n",
    "            # объединяем словари\n",
    "            dict_row = merge_two_dicts(dict_row, dict_part_row)\n",
    "        \n",
    "    return (dict_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tyQ-WQFi_kOe"
   },
   "outputs": [],
   "source": [
    "def getPageLinks(link, n_page):\n",
    "    '''\n",
    "    Возвращает список ссылок на странице выдачи\n",
    "    link - ссылка на страницу поисковой выдачи\n",
    "    n_page: int/string - номер страницы для парсинга\n",
    "    '''\n",
    "    auto_links = []\n",
    "    # составляем ссылку на страницу поиска\n",
    "    page_link = link + '&page=' + str(n_page)\n",
    "\n",
    "    response = requests.get(page_link)\n",
    "    if not response.ok: # если сервер нам отказал, вернем статус ошибки \n",
    "        print(f'Сервер: {response.status_code}, {link}')\n",
    "    else:\n",
    "        # получаем содержимое страницы\n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "        # ищем ссылки и очищаем их от ненужных тэгов\n",
    "        auto_links = soup.find_all('a', class_='Link ListingItemTitle__link')\n",
    "        auto_links = [link.attrs['href'] for link in auto_links]\n",
    "\n",
    "    return (auto_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "thHkfUUd_kOf"
   },
   "outputs": [],
   "source": [
    "def getColor(soup):\n",
    "    try:\n",
    "        block = soup.find('li',class_='CardInfoRow CardInfoRow_color')\n",
    "        color = block.find('a').text\n",
    "        # color = block.find('a', class_='Link_color_black').text\n",
    "    except:\n",
    "        color = ''\n",
    "    return (color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VlhM0MR8_kOf"
   },
   "outputs": [],
   "source": [
    "def getFoto(soup):\n",
    "    try:\n",
    "        images = soup.find_all('img',\n",
    "                               {'class': 'ImageGalleryDesktop__image ImageGalleryDesktop__image_hidden'})\n",
    "        foto_links = [img.get('src') for img in images]\n",
    "    except:\n",
    "        foto_links=[]\n",
    "    return (foto_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "au4IPtI1_kOg"
   },
   "outputs": [],
   "source": [
    "def getSkript_initial_state(soup, list_features, url):\n",
    "    '''\n",
    "    Возвращает словарь значений признаков\n",
    "    soup - html-страница с объявлением\n",
    "    list_features - список названий признаков\n",
    "    '''\n",
    "    my_dict ={}\n",
    "    try:\n",
    "        json_data = json.loads(soup.find('script', attrs={'id': 'initial-state'}).string)\n",
    "        for i in list_features:\n",
    "            for t in find_key(json_data, i):\n",
    "                my_dict[i] = t[1]\n",
    "                if i == 'model_info':# берём  model_name из словаря model_info\n",
    "                    my_dict['model_name'] = t[1]['code']\n",
    "    except AttributeError:\n",
    "        print(f'Ошибка: {url}')\n",
    "        \n",
    "    return (my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MFlWZQLt_kOh"
   },
   "outputs": [],
   "source": [
    "def getSkript_application_ld_json(soup,list_features, url):\n",
    "    '''\n",
    "    Возвращает словарь значений признаков\n",
    "    soup - html-страница с объявлением\n",
    "    list_features - список названий признаков\n",
    "    '''\n",
    "    my_dict ={}\n",
    "    try:\n",
    "        all_blocks = soup.find_all('script',type='application/ld+json')\n",
    "        for block in all_blocks:\n",
    "            json_data = json.loads(block.string)\n",
    "            for i in list_features:\n",
    "                for t in find_key(json_data, i):\n",
    "                    my_dict[i] = t[1]\n",
    "    except AttributeError:\n",
    "        print(f'Ошибка: {url}')\n",
    "                \n",
    "    return (my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XRW0i6SM_kOh"
   },
   "outputs": [],
   "source": [
    "def getСharacteristics(soup, list_features):\n",
    "    '''\n",
    "    Возвращает со страницы автомобиля значение заданного параметра:\n",
    "    soup - веб-страница\n",
    "    sign - изменяемая часть наименования класса списка\n",
    "    '''\n",
    "    # словарь подстановок для парсинга\n",
    "    dict_param = {'Привод':'drive','Руль':'wheel','Состояние':'state','Владельцы':'ownersCount',\n",
    "                  'ПТС':'pts','Таможня':'customs','Владение':'owningTime'}\n",
    "    my_dict ={}\n",
    "    for feature in list_features:\n",
    "        if feature in dict_param:\n",
    "            cl = 'CardInfoRow CardInfoRow_' + str(dict_param[feature])\n",
    "            try:\n",
    "                value = soup.find('li', class_=cl).contents[1].text\n",
    "            except:\n",
    "                value = ''\n",
    "            my_dict[feature] = value\n",
    "            \n",
    "    return (my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HlPmJoos_kOh"
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    '''\n",
    "    Объединение словарей\n",
    "    '''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XtGTkE_P_kOi"
   },
   "outputs": [],
   "source": [
    "def SaveToExcel(df, name):\n",
    "    with pd.ExcelWriter(name) as writer:\n",
    "        df.to_excel(writer, sheet_name='Лист 1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rcmGzcKU_kOi"
   },
   "outputs": [],
   "source": [
    "# Рекурсивные генераторы для поиска по объекту, состоящему из словарей и списков\n",
    "def find_key(obj, key):\n",
    "    if isinstance(obj, dict):\n",
    "        yield from iter_dict(obj, key, [])\n",
    "    elif isinstance(obj, list):\n",
    "        yield from iter_list(obj, key, [])\n",
    "\n",
    "def iter_dict(d, key, indices):\n",
    "    for k, v in d.items():\n",
    "        if k == key:\n",
    "            yield indices + [k], v\n",
    "        if isinstance(v, dict):\n",
    "            yield from iter_dict(v, key, indices + [k])\n",
    "        elif isinstance(v, list):\n",
    "            yield from iter_list(v, key, indices + [k])\n",
    "\n",
    "def iter_list(seq, key, indices):\n",
    "    for k, v in enumerate(seq):\n",
    "        if isinstance(v, dict):\n",
    "            yield from iter_dict(v, key, indices + [k])\n",
    "        elif isinstance(v, list):\n",
    "            yield from iter_list(v, key, indices + [k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T3WnmZV_kOi"
   },
   "source": [
    "# Списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BMV4Yzm8_kOj"
   },
   "outputs": [],
   "source": [
    "original_columns = ['bodyType', 'brand', 'car_url', 'color', 'complectation_dict', 'description', 'engineDisplacement', \n",
    "                    'enginePower', 'equipment_dict', 'fuelType', 'image', 'mileage', 'modelDate', 'model_info', \n",
    "                    'model_name', 'name', 'numberOfDoors', 'parsing_unixtime', 'priceCurrency', 'productionDate', \n",
    "                    'sell_id', 'super_gen', 'vehicleConfiguration', 'vehicleTransmission', 'vendor', 'Владельцы', \n",
    "                    'Владение', 'ПТС', 'Привод', 'Руль', 'Состояние', 'Таможня']\n",
    "original_columns = original_columns + ['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVrv99fx_kOj"
   },
   "source": [
    "Некоторые признаки из тестового датафрейма (по ошибке, либо в силу времени) в структуре сайта называются иначе, либо отсутсвуют. Создадим словарь для переименованных признаков, так будет проще работать и вернуться к первоначальному состоянию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "R_2CExdW_kOj"
   },
   "outputs": [],
   "source": [
    "# словарь переименований\n",
    "dict_rename = {\n",
    "    'complectation_dict':'complectation',\n",
    "    'equipment_dict':'equipment',\n",
    "    'name':'human_name',\n",
    "    'sell_id':'sale_id',\n",
    "    'super_gen':'tech_param'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tsvAKAgf_kOj"
   },
   "outputs": [],
   "source": [
    "# Теперь создадим список параметров, с которым будем работать\n",
    "work_list = [dict_rename[key] if dict_rename.get(key) else key for key in original_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vm9KVgr5_kOk"
   },
   "source": [
    "# Парсер\n",
    "Лучше запускать по 1 марке параллельно в разных ноутбуках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NIUuqYrM_kOk"
   },
   "outputs": [],
   "source": [
    "# # Список марок для парсинга\n",
    "# list_brands = ['LEXUS','MERCEDES','BMW','MITSUBISHI',\n",
    "#                'TOYOTA','SKODA','HONDA','VOLVO','AUDI','VOLKSWAGEN','INFINITI','NISSAN']\n",
    "\n",
    "list_brands = ['INFINITI']\n",
    "# Задаём начальные параметры для парсинга (бренды, интервал дат, признаки)\n",
    "df = getData(list_brands, 2021, 2021, work_list)\n",
    "\n",
    "# вернём названия в прежнее состояние\n",
    "for key in dict_rename:\n",
    "    if dict_rename[key] in df.columns:\n",
    "        df.rename({dict_rename[key]: key}, axis=1, inplace=True)\n",
    "df = df[original_columns]\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Необработанный формат ячейки",
  "colab": {
   "collapsed_sections": [],
   "name": "parser_auto_ru.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
