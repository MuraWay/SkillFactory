{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Необработанный формат ячейки",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "parser_auto_ru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkAnkDOi_kOW"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import ast\n",
        "import collections\n",
        "import datetime\n",
        "import json\n",
        "from pprint import pprint\n",
        "from tqdm import tnrange\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfstFsTl_kOa"
      },
      "source": [
        "# Функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjTLWZo2_kOb"
      },
      "source": [
        "def getData(brands, start_year, finish_year, list_feature):\n",
        "    '''\n",
        "    Возвращает датафрейм со спарсенными данными\n",
        "    brands (str/list) - список марок автомобилей для парсинга\n",
        "    min_year (int) - минимальный год выпуска авто, который нас интересует\n",
        "    list_feature (list) - список признаков для парсинга\n",
        "\n",
        "    Ограничение avto.ru - 3700 ссылок за 1 раз, поэтому по каждому бренду и \n",
        "    году будем делать отдельный запрос\n",
        "    \n",
        "    Задаём начальный фильт выдачи:\n",
        "    с пробегом, фото - не важно, таможня - не важно, состояние не важно\n",
        "    ссылка копируется с сайта\n",
        "    '''       \n",
        "    # начальная страница - фильтр\n",
        "    url = 'https://auto.ru/cars/used/?damage_group=ANY&customs_state_group\\=DOESNT_MATTER&has_image=false'\n",
        "\n",
        "    # если вдруг на входе одна марка, а не список \n",
        "    if type(brands)==str:\n",
        "        brands = brands.split()\n",
        "\n",
        "    # иницализируем датафрейм\n",
        "    df = pd.DataFrame(columns=list_feature)\n",
        "    \n",
        "    # на случай, если парсить по возрастанию или убыванию даты выпуска\n",
        "    if start_year > finish_year:\n",
        "        step = -1\n",
        "    else: \n",
        "        step = 1\n",
        "    finish_year += step\n",
        "\n",
        "    # визуализируем прогресс выполнения\n",
        "    print(list_brands)\n",
        "    with tnrange(len(list_brands)) as brands:\n",
        "        for brand in brands:\n",
        "            brands.set_description(f'Общий ход выполнения')\n",
        "            \n",
        "            # Собираем все ссылки по бренду в заданном интервале дат:\n",
        "            all_links = getAllLinksBrand(url, start_year, finish_year, step, list_brands[brand])\n",
        "    \n",
        "            with tnrange(len(all_links), leave=True, unit=' url ') as links:\n",
        "                for lks in links:\n",
        "                    links.set_description(f'{list_brands[brand]}')\n",
        "                    \n",
        "                    # переходим по найденным на странице ссылкам собираем данные\n",
        "                    data_row = getAutoData(all_links[lks], list_feature)\n",
        "                    df = df.append(data_row, ignore_index=True)\n",
        "\n",
        "            # Сохраняем (после каждого бренда на всякий случай)\n",
        "            file_name = f'{brand}_{start_year}_{finish_year}.csv'\n",
        "            df.to_csv(file_name, sep='\\t', index=False, encoding='utf-8')\n",
        "\n",
        "            # Альтернативно\n",
        "            # file_name = ('parser-{date:%Y-%m-%d_%H_%M_%S}.xlsx'.\n",
        "            #              format( date=datetime.datetime.now()))\n",
        "            # SaveToExcel(df,file_name)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-79fEzd_kOc"
      },
      "source": [
        "def getAllLinksBrand(url, start_year, finish_year, step, brand):\n",
        "    '''\n",
        "    возвращает список ссылок по бренду в интервале дат\n",
        "    '''\n",
        "    all_links = [] # список для всех ссылок по бренду\n",
        "    # из-за ограничения собираем все возможные ссылки по 1 бренду и году за раз\n",
        "    \n",
        "\n",
        "    with tnrange(start_year,finish_year, step, leave=False) as years:\n",
        "        for year in years:\n",
        "            years.set_description(f'{year}')\n",
        "            # динамическая часть ссылки\n",
        "            part = str(brand).lower() + '/' + str(year) + '-year'\n",
        "            # собираем ссылку для поиска по одному бренду и году \n",
        "            link = url.replace('cars/used','cars/' + part + '/used')\n",
        "            \n",
        "            # собираем ссылки с каждой страницы выдачи (максимум - 99 страниц)\n",
        "            for n_page in range(1,100):\n",
        "                page_links = getPageLinks(link, n_page) # список ссылок с текущей страницы\n",
        "                if len(page_links) == 0: # выходим если не нашли ссылок\n",
        "                    break\n",
        "                else:\n",
        "                    all_links += page_links\n",
        "    return (all_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OLIncTF_kOc"
      },
      "source": [
        "def getAutoData(auto_page, list_feature):\n",
        "    '''\n",
        "    Запрашивает данные по странице, возвращает обработанный словарь с данными\n",
        "    auto_page (string) ссылка на страницу с объявлением\n",
        "    '''   \n",
        "    # словарь, для храения значений спарсенных значений признаков\n",
        "    dict_row = {}\n",
        "    \n",
        "    # запрашиваем данные по ссылке , headers={'User-Agent': UserAgent().chrome}\n",
        "    response = requests.get(auto_page)\n",
        "    \n",
        "    if not response.ok: # если сервер нам отказал, вернем статус ошибки \n",
        "        print(f'Сервер: {response.status_code}, {auto_page}')\n",
        "    else:  \n",
        "        soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "        # динамический список признаков для парсинга\n",
        "        list_snippet = set(list_feature.copy())\n",
        "\n",
        "        # используем функции для парсинга и вручную добавляем еще 2\n",
        "        for func in range(6):\n",
        "            if func == 0:\n",
        "                dict_part_row = {'color':getColor(soup)}\n",
        "            if func == 1:\n",
        "                dict_part_row = {'image':getFoto(soup)}\n",
        "            if func == 2:\n",
        "                dict_part_row = getSkript_initial_state(soup, list_snippet, auto_page)\n",
        "            elif func == 3:\n",
        "                dict_part_row = getSkript_application_ld_json(soup, list_snippet, auto_page)\n",
        "            elif func == 4:\n",
        "                dict_part_row = getСharacteristics(soup, list_snippet)\n",
        "            else:\n",
        "                timestamp = int(time.time())\n",
        "                dict_part_row = {'car_url':auto_page,\n",
        "                                 'parsing_unixtime':timestamp}\n",
        "\n",
        "            # исключим из списка ту часть признаков, для которой получены значения\n",
        "            llist = set(list(dict_part_row.keys()))\n",
        "            list_snippet -= llist\n",
        "\n",
        "            # объединяем словари\n",
        "            dict_row = merge_two_dicts(dict_row, dict_part_row)\n",
        "        \n",
        "    return (dict_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyQ-WQFi_kOe"
      },
      "source": [
        "def getPageLinks(link, n_page):\n",
        "    '''\n",
        "    Возвращает список ссылок на странице выдачи\n",
        "    link - ссылка на страницу поисковой выдачи\n",
        "    n_page: int/string - номер страницы для парсинга\n",
        "    '''\n",
        "    auto_links = []\n",
        "    # составляем ссылку на страницу поиска\n",
        "    page_link = link + '&page=' + str(n_page)\n",
        "\n",
        "    response = requests.get(page_link)\n",
        "    if not response.ok: # если сервер нам отказал, вернем статус ошибки \n",
        "        print(f'Сервер: {response.status_code}, {link}')\n",
        "    else:\n",
        "        # получаем содержимое страницы\n",
        "        soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "        # ищем ссылки и очищаем их от ненужных тэгов\n",
        "        auto_links = soup.find_all('a', class_='Link ListingItemTitle__link')\n",
        "        auto_links = [link.attrs['href'] for link in auto_links]\n",
        "\n",
        "    return (auto_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thHkfUUd_kOf"
      },
      "source": [
        "def getColor(soup):\n",
        "    try:\n",
        "        block = soup.find('li',class_='CardInfoRow CardInfoRow_color')\n",
        "        color = block.find('a').text\n",
        "        # color = block.find('a', class_='Link_color_black').text\n",
        "    except:\n",
        "        color = ''\n",
        "    return (color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlhM0MR8_kOf"
      },
      "source": [
        "def getFoto(soup):\n",
        "    try:\n",
        "        images = soup.find_all('img',\n",
        "                               {'class': 'ImageGalleryDesktop__image ImageGalleryDesktop__image_hidden'})\n",
        "        foto_links = [img.get('src') for img in images]\n",
        "    except:\n",
        "        foto_links=[]\n",
        "    return (foto_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au4IPtI1_kOg"
      },
      "source": [
        "def getSkript_initial_state(soup, list_features, url):\n",
        "    '''\n",
        "    Возвращает словарь значений признаков\n",
        "    soup - html-страница с объявлением\n",
        "    list_features - список названий признаков\n",
        "    '''\n",
        "    my_dict ={}\n",
        "    try:\n",
        "        json_data = json.loads(soup.find('script', attrs={'id': 'initial-state'}).string)\n",
        "        for i in list_features:\n",
        "            for t in find_key(json_data, i):\n",
        "                my_dict[i] = t[1]\n",
        "                if i == 'model_info':# берём  model_name из словаря model_info\n",
        "                    my_dict['model_name'] = t[1]['code']\n",
        "    except AttributeError:\n",
        "        print(f'Ошибка: {url}')\n",
        "        \n",
        "    return (my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFlWZQLt_kOh"
      },
      "source": [
        "def getSkript_application_ld_json(soup,list_features, url):\n",
        "    '''\n",
        "    Возвращает словарь значений признаков\n",
        "    soup - html-страница с объявлением\n",
        "    list_features - список названий признаков\n",
        "    '''\n",
        "    my_dict ={}\n",
        "    try:\n",
        "        all_blocks = soup.find_all('script',type='application/ld+json')\n",
        "        for block in all_blocks:\n",
        "            json_data = json.loads(block.string)\n",
        "            for i in list_features:\n",
        "                for t in find_key(json_data, i):\n",
        "                    my_dict[i] = t[1]\n",
        "    except AttributeError:\n",
        "        print(f'Ошибка: {url}')\n",
        "                \n",
        "    return (my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRW0i6SM_kOh"
      },
      "source": [
        "def getСharacteristics(soup, list_features):\n",
        "    '''\n",
        "    Возвращает со страницы автомобиля значение заданного параметра:\n",
        "    soup - веб-страница\n",
        "    sign - изменяемая часть наименования класса списка\n",
        "    '''\n",
        "    # словарь подстановок для парсинга\n",
        "    dict_param = {'Привод':'drive','Руль':'wheel','Состояние':'state','Владельцы':'ownersCount',\n",
        "                  'ПТС':'pts','Таможня':'customs','Владение':'owningTime'}\n",
        "    my_dict ={}\n",
        "    for feature in list_features:\n",
        "        if feature in dict_param:\n",
        "            cl = 'CardInfoRow CardInfoRow_' + str(dict_param[feature])\n",
        "            try:\n",
        "                value = soup.find('li', class_=cl).contents[1].text\n",
        "            except:\n",
        "                value = ''\n",
        "            my_dict[feature] = value\n",
        "            \n",
        "    return (my_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlPmJoos_kOh"
      },
      "source": [
        "def merge_two_dicts(x, y):\n",
        "    '''\n",
        "    Объединение словарей\n",
        "    '''\n",
        "    z = x.copy()\n",
        "    z.update(y)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGTkE_P_kOi"
      },
      "source": [
        "def SaveToExcel(df, name):\n",
        "    with pd.ExcelWriter(name) as writer:\n",
        "        df.to_excel(writer, sheet_name='Лист 1', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcmGzcKU_kOi"
      },
      "source": [
        "# Рекурсивные генераторы для поиска по объекту, состоящему из словарей и списков\n",
        "def find_key(obj, key):\n",
        "    if isinstance(obj, dict):\n",
        "        yield from iter_dict(obj, key, [])\n",
        "    elif isinstance(obj, list):\n",
        "        yield from iter_list(obj, key, [])\n",
        "\n",
        "def iter_dict(d, key, indices):\n",
        "    for k, v in d.items():\n",
        "        if k == key:\n",
        "            yield indices + [k], v\n",
        "        if isinstance(v, dict):\n",
        "            yield from iter_dict(v, key, indices + [k])\n",
        "        elif isinstance(v, list):\n",
        "            yield from iter_list(v, key, indices + [k])\n",
        "\n",
        "def iter_list(seq, key, indices):\n",
        "    for k, v in enumerate(seq):\n",
        "        if isinstance(v, dict):\n",
        "            yield from iter_dict(v, key, indices + [k])\n",
        "        elif isinstance(v, list):\n",
        "            yield from iter_list(v, key, indices + [k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T3WnmZV_kOi"
      },
      "source": [
        "# Списки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMV4Yzm8_kOj"
      },
      "source": [
        "original_columns = ['bodyType', 'brand', 'car_url', 'color', 'complectation_dict', 'description', 'engineDisplacement', \n",
        "                    'enginePower', 'equipment_dict', 'fuelType', 'image', 'mileage', 'modelDate', 'model_info', \n",
        "                    'model_name', 'name', 'numberOfDoors', 'parsing_unixtime', 'priceCurrency', 'productionDate', \n",
        "                    'sell_id', 'super_gen', 'vehicleConfiguration', 'vehicleTransmission', 'vendor', 'Владельцы', \n",
        "                    'Владение', 'ПТС', 'Привод', 'Руль', 'Состояние', 'Таможня']\n",
        "original_columns = original_columns + ['price']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVrv99fx_kOj"
      },
      "source": [
        "Некоторые признаки из тестового датафрейма (по ошибке, либо в силу времени) в структуре сайта называются иначе, либо отсутсвуют. Создадим словарь для переименованных признаков, так будет проще работать и вернуться к первоначальному состоянию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_2CExdW_kOj"
      },
      "source": [
        "# словарь переименований\n",
        "dict_rename = {\n",
        "    'complectation_dict':'complectation',\n",
        "    'equipment_dict':'equipment',\n",
        "    'name':'human_name',\n",
        "    'sell_id':'sale_id',\n",
        "    'super_gen':'tech_param'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsvAKAgf_kOj"
      },
      "source": [
        "# Теперь создадим список параметров, с которым будем работать\n",
        "work_list = [dict_rename[key] if dict_rename.get(key) else key for key in original_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm9KVgr5_kOk"
      },
      "source": [
        "# Парсер\n",
        "Лучше запускать по 1 марке параллельно в разных ноутбуках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIUuqYrM_kOk"
      },
      "source": [
        "# Список марок для парсинга\n",
        "list_brands = ['LEXUS','MERCEDES','BMW','MITSUBISHI',\n",
        "               'TOYOTA','SKODA','HONDA','VOLVO','AUDI','VOLKSWAGEN','INFINITI','NISSAN']\n",
        "\n",
        "# Задаём начальные параметры для парсинга (бренды, интервал дат, признаки)\n",
        "df = getData(list_brands, 2011, 2021, work_list)\n",
        "\n",
        "# вернём названия в прежнее состояние\n",
        "for key in dict_rename:\n",
        "    if dict_rename[key] in df.columns:\n",
        "        df.rename({dict_rename[key]: key}, axis=1, inplace=True)\n",
        "df = df[original_columns]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}